Saliency Heatmaps in macOS 26 “Tahoe” Vision Framework

How Vision’s Saliency Detection Works (High-Level Overview)

Apple’s Vision framework (available in macOS 26 Tahoe and recent iOS versions) can analyze an image and generate a saliency heatmap – essentially an overlay highlighting the parts of the image that are most likely to capture a viewer’s attention ￼. In simpler terms, it tries to figure out “what stands out” in the picture, the areas your eyes would naturally be drawn to first.

The Vision API provides two types of saliency analysis:
    •    Attention-based saliency: This method predicts the regions that would catch a human’s eye at first glance. It was trained using data on where people actually look in photos (for example, from eye-tracking studies), so it factors in things like faces, areas of high contrast, bright colors, and strong lighting that tend to grab attention ￼ ￼. In other words, it imitates human visual attention – if there’s a face or a brightly colored subject in the scene, the attention-based heatmap will light up those spots.
    •    Objectness-based saliency: This method highlights prominent objects in the image, whether or not they are the first thing a human would look at. It was trained on object segmentation tasks, meaning it seeks distinct objects or shapes (like a tree, an animal, a building, etc.) that stand out from the background ￼. For example, if you have a landscape with a deer standing in a field, the objectness model will try to mark the deer (and possibly other notable objects like a solitary tree or rock) as salient regions simply because they are tangible objects in the scene.

Both saliency approaches ultimately produce a heatmap (sometimes called a saliency map) as output. This heatmap is essentially a low-resolution grid (the Vision framework uses about a 68×68 pixel buffer by default) where each cell’s intensity represents how “salient” or attention-grabbing that part of the image is ￼. The heatmap is typically delivered as a grayscale image (or it can be visualized in false colors) that you can overlay on the original. Bright or hot areas on the map indicate regions the algorithm deems most interesting, while dark areas indicate regions that likely fade into the background ￼. You can imagine it like shining a spotlight on the key subjects: the brighter the spotlight on a part of the image, the more it stands out from its surroundings.

Example: An attention-based saliency heatmap overlaid on an image. The bright areas (right image) show where the model predicts a viewer’s gaze will focus – in this case, the dessert’s bright yellow topping stands out most, while the background is dimmed out. Such a heatmap highlights the image’s most eye-catching features.

Internally, the Vision framework uses machine learning models to generate these maps. The attention-based model likely leverages a deep neural network trained on thousands of images with human eye-tracking data, so it has learned common patterns of human attention (for instance, people tend to look at faces or animals, text, high-contrast edges, saturated colors, etc. first) ￼. The objectness-based model is more of a general object detector – it was trained to find any distinct foreground objects in an image by looking at differences in color/texture or learned object features, even if those objects are not “interesting” in a human sense ￼. In practice, the two often overlap (because things that are isolated objects are often also attention-grabbing), but the attention-based saliency is better at always returning something (even in a scene without obvious objects) by focusing on visual uniqueness, whereas the objectness model might say “no salient object found” if the image is very uniform (e.g. a blank sky) ￼ ￼.

Saliency Map Outputs and Scoring in the Vision API

When you run a saliency request on an image using the Vision framework, you get back a VNSaliencyImageObservation object. This contains two important pieces of data: the heatmap itself and an optional list of salient regions. The heatmap is a machine-readable image (a pixel buffer) where each pixel’s value indicates saliency at that location ￼. In a developer context, you can retrieve this and overlay it as we discussed, but you can also examine the raw values. For instance, you could find the maximum saliency value or the average – theoretically, a higher peak value might indicate a more strongly focused image. There isn’t a single “attention score” number that the API directly gives for the whole image, but since the heatmap is essentially a field of saliency scores, a developer could derive metrics from it (like “our main subject has saliency 0.9 on a scale of 0–1, versus the background at 0.2”).

The Vision API also identifies specific salient regions within the image. These come in the form of bounding boxes with associated confidence values. In code, this is the salientObjects array, which contains one or more VNRectangleObservation items ￼. Each such rectangle is essentially the algorithm saying “I think this box contains an important feature” and it comes with a confidence score or saliency score indicating how strongly that region stood out. In practical terms, you can use these to, say, draw a box around the most salient object or crop the image to the region of interest. For example, in an image with multiple interesting bits, the objectness-based analysis might return up to three top salient object regions (Apple’s API limits it to about three likely objects) ￼, each with its own confidence score. The regions are typically ranked by saliency, so the first one in the array would be the most salient object, the next one somewhat less, and so on. The attention-based method often yields just one dominant region (since it’s trying to pinpoint the single area a viewer would fixate on first) ￼.

To answer the question directly: yes, you can get a form of scoring or ranking from the saliency output. Instead of a single numeric “score” for the whole image, you get a map of scores (the heatmap) and/or a set of salient regions each with a confidence value. The confidence values act like a ranking of how salient each detected region is ￼. For instance, if the Vision framework finds two salient regions (say, one is a bright flower and another is a bird in the background), it might give the flower a higher saliency confidence than the bird, indicating the flower is more attention-grabbing. If an image has no particularly salient features (for example, a very uniform landscape with nothing that pops out), the salientObjects list can come back empty ￼ – effectively telling you the image has no strong points of interest identified. In that case, the heatmap would be fairly flat (no bright spots), which itself is a kind of informative “score” (it suggests the image might not have a clear focal point, or is possibly low-contrast or blurry). In fact, one noted application of these saliency metrics is blur detection: a very blurry photo tends to have low, uniform saliency values since nothing in it really stands out sharply ￼.

Uses of Saliency Maps for Nature & Landscape Photographers

Saliency maps aren’t just for developers – they can be a handy analytical tool for photographers as well. Having a heatmap of which parts of a photo draw attention can provide insights into composition and image impact. Here are some useful ways nature and landscape photographers might leverage saliency maps on their images:
    •    Composition feedback and focal-point check: A saliency map can tell you if your intended subject is indeed the star of the photo. For example, if you took a landscape photo where the main subject is a distant mountain, the saliency heatmap will show whether that mountain actually catches the eye or if something else (maybe a brightly colored object in the foreground or an oddly placed sign) is unintentionally stealing the viewer’s attention. This is like getting a peek into a viewer’s mind – you learn what pops out at first glance. If the heatmap’s brightest spot isn’t where you expected, you might reconsider your composition or edit the photo to guide the viewer better (for instance, by darkening or removing distracting areas). Photographers often strive to have a clear focal point in an image, and the saliency map is a quick “at-a-glance” test for that. In studies of photographic aesthetics, images with a well-defined, isolated area of interest (with other elements not competing as much) tend to be more appealing, and saliency maps help visualize that balance.
    •    Identifying and removing distractions: In nature and landscape photography, you can sometimes have unwanted attention-grabbers – maybe a brightly colored trash can at the edge of a forest scene, or a lens flare in the sky, or a person in a red jacket walking through an otherwise serene green landscape. These elements might not be obvious when you’re focused on the grand scene, but a saliency analysis will immediately highlight them if they draw the eye. By looking at the heatmap, you might discover, “Oh, that bright rock in the corner is surprisingly salient.” With that knowledge, you can go back and clone it out or crop it away. Essentially, saliency maps can act as a second pair of eyes, pointing out bits of the photo that shout for attention. This can improve your post-processing: you could selectively tone down or blur the distracting areas and thereby direct focus back to your main subject.
    •    Image selection and quality control: If you have a whole batch of shots (say you took many frames of a sunset or a wildlife scene), saliency metrics could help in picking the best shots. You might use an algorithm to rank images by how strongly they have a single clear salient region. Photos where one subject clearly stands out (high contrast relative to background) could be deemed more impactful than those where nothing particularly stands out. On the flip side, if an image’s saliency map is very flat (no peaks), it might indicate the photo is bland, out-of-focus, or cluttered. In fact, as mentioned, a blurry or low-detail photo will result in weak saliency overall – nothing in it catches attention ￼. A photographer could automatically flag such images for review or discard. This doesn’t replace your artistic judgment, of course, but it can be a helpful automated “first pass” to identify images that lack a clear subject or have technical issues. It’s a bit like having an assistant sort photos by which ones have punch and which ones look dull at a glance.
    •    Automated cropping and framing suggestions: Saliency data can be used to suggest how to crop a photo for better composition. For example, Apple’s documentation itself notes that one can use the detected salient region to crop the image down to just the interesting part ￼. Suppose you have a wide landscape shot and within it there’s a small but very salient element (like a lone tree or an animal). A saliency-aware tool could recommend a tighter crop around that element, turning a wide shot into a more impactful close-up. This could be useful for creating thumbnails or previews of your photos that really showcase the subject. Similarly, if you’re preparing an image for, say, an Instagram crop (which is square or 4:5), the heatmap could guide you on how to position the crop so that the most salient piece stays in frame. Photographers can of course do this by eye, but the saliency map provides an objective map of interest across the image. It’s like having a built-in guide for the rule of thirds or centering – if the saliency hotspot is oddly off to one side or cut off, you might want to re-frame. (Interestingly, many pleasing photos tend to have the main salient region not too close to the edges – some research even found a preference for when the salient object is near the center or at power points in the frame. The heatmap can help visualize if you achieved that balance.)
    •    Guiding selective editing: Beyond cropping, knowing the saliency distribution lets you edit more intelligently. For instance, a nature photographer might use the saliency map as a mask to apply adjustments: you could boost sharpness or clarity on the salient region (to enhance the subject), and maybe gently reduce the saturation or exposure in non-salient areas (to further subdue any background distractions). Because the Vision framework’s saliency can output a mask of important pixels, this could potentially be done automatically. Another creative use-case could be slideshows or animations: if you know where the interesting part of each image is, you could program a slideshow to pan toward that area (a kind of automated “Ken Burns effect” focusing on the point of interest). This way, even a static landscape photo can have an animated emphasis on what matters. All these ideas stem from the core benefit that saliency maps tell you where the visual weight of the image lies, which is incredibly useful information for making compositional and editing decisions.

In summary, saliency maps in macOS 26’s Vision framework are a powerful feature that identify what pops out in an image. They work by using trained models to simulate human attention and object detection, producing a heatmap (and region data) of interesting areas. While primarily a developer tool, the insights from saliency analysis can absolutely be used by nature and landscape photographers to evaluate and improve their photos – from checking if the subject commands attention, to finding distractions, selecting the best images, and guiding edits or crops. It’s all done on-device (no internet needed), so a macOS app could churn through a photographer’s library and generate these maps offline. Even if you’re not sure yet what to do with the data, having it available opens up a lot of creative and practical possibilities to refine your photography workflow based on what truly stands out in each shot.

Sources:
    1.    Apple Vision Framework – Saliency API and definitions ￼ ￼ ￼ ￼
    2.    Tiago Gomes Pereira, Identifying attention areas in images with Vision (Create with Swift, Oct 2024) ￼ ￼
    3.    Anupam Chugh, Cropping Areas of Interest Using Vision in iOS (Better Programming, 2019) ￼ ￼
    4.    Kamil Tustanowski, Saliency detection using the Vision framework (Medium, 2023) ￼ ￼
    5.    Fritz AI, Advancements in Apple’s Vision Framework (WWDC 2019 highlights) ￼ ￼
